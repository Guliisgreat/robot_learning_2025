{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/milarobotlearningcourse/robot_learning_2025.git\n",
    "%cd robot_learning_2025/hw2\n",
    "!pip3 install -r requirements.txt\n",
    "\n",
    "# Install repo\n",
    "import cv2\n",
    "import tensorflow_datasets as tfds\n",
    "import tqdm\n",
    "import rlds, numpy as np\n",
    "import mediapy as media\n",
    "from PIL import Image\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython import display\n",
    "def as_gif(images, path='temp.gif'):\n",
    "  # Render the images as the gif:\n",
    "  images = [Image.fromarray(image) for image in images]\n",
    "  images[0].save(path, save_all=True, append_images=images[1:], duration=1000, loop=0)\n",
    "  gif_bytes = open(path,'rb').read()\n",
    "  return gif_bytes\n",
    "\n",
    "# create RLDS dataset builder\n",
    "builder = tfds.builder_from_directory(builder_dir='gs://gresearch/robotics/bridge/0.1.0/')\n",
    "dataset = builder.as_dataset(split='train[:1]')\n",
    "\n",
    "# sample episode + resize to 256x256 (default third-person cam resolution)\n",
    "episode = next(iter(dataset))\n",
    "print(episode)\n",
    "\n",
    "steps = list(episode['steps'])\n",
    "images = np.array([cv2.resize(np.array(step['observation']['image']), (256, 256)) for step in steps])\n",
    "\n",
    "# extract goal image & language instruction\n",
    "goal_image = images[-1]\n",
    "language_instruction = steps[0]['observation']['natural_language_instruction'].numpy().decode()\n",
    "\n",
    "# visualize episode\n",
    "print(f'Instruction: {language_instruction}')\n",
    "display.Image(as_gif(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Looking at the actions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "actions = np.array([np.array(step['action']['world_vector']) for step in steps])\n",
    "\n",
    "plt.plot(actions[:, 0], label='x')\n",
    "plt.plot(actions[:, 1], label='y')\n",
    "plt.plot(actions[:, 2], label='z')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install SimpleEnv for evaluation in the simulation\n",
    "# @title Install vulkan for rendering\n",
    "!apt-get install -yqq --no-install-recommends libvulkan-dev vulkan-tools\n",
    "# below fixes some bugs introduced by some recent Colab changes\n",
    "!mkdir -p /usr/share/vulkan/icd.d\n",
    "!wget -q -P /usr/share/vulkan/icd.d https://raw.githubusercontent.com/haosulab/ManiSkill/main/docker/nvidia_icd.json\n",
    "!wget -q -O /usr/share/glvnd/egl_vendor.d/10_nvidia.json https://raw.githubusercontent.com/haosulab/ManiSkill/main/docker/10_nvidia.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Make sure vulkan is installed correctly\n",
    "!vulkaninfo | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/milarobotlearningcourse/SimplerEnv.git --recurse-submodules\n",
    "!pip install -e ./SimplerEnv/ManiSkill2_real2sim/\n",
    "!pip install -e ./SimplerEnv\n",
    "!mkdir ./SimplerEnv/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simpler_env\n",
    "from simpler_env.utils.env.observation_utils import get_image_from_maniskill2_obs_dict\n",
    "import mediapy\n",
    "import sapien.core as sapien\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "task_name = \"widowx_carrot_on_plate\"  # @param [\"google_robot_pick_coke_can\", \"google_robot_move_near\", \"google_robot_open_drawer\", \"google_robot_close_drawer\", \"widowx_spoon_on_towel\", \"widowx_carrot_on_plate\", \"widowx_stack_cube\", \"widowx_put_eggplant_in_basket\"]\n",
    "\n",
    "if 'env' in locals():\n",
    "  print(\"Closing existing env\")\n",
    "  env.close()\n",
    "  del env\n",
    "env = simpler_env.make(task_name)\n",
    "# Colab GPU does not support denoiser\n",
    "sapien.render_config.rt_use_denoiser = False\n",
    "obs, reset_info = env.reset()\n",
    "instruction = env.get_language_instruction()\n",
    "print(\"Reset info\", reset_info)\n",
    "print(\"Instruction\", instruction)\n",
    "\n",
    "frames = []\n",
    "done, truncated = False, False\n",
    "while not (done or truncated):\n",
    "   # action[:3]: delta xyz; action[3:6]: delta rotation in axis-angle representation;\n",
    "   # action[6:7]: gripper (the meaning of open / close depends on robot URDF)\n",
    "   image = get_image_from_maniskill2_obs_dict(env, obs)\n",
    "   action = env.action_space.sample() # replace this with your policy inference\n",
    "   obs, reward, done, truncated, info = env.step(action)\n",
    "   frames.append(image)\n",
    "\n",
    "episode_stats = info.get('episode_stats', {})\n",
    "print(\"Episode stats\", episode_stats)\n",
    "# mediapy.show_video(frames, fps=10)\n",
    "import moviepy.editor as mpy\n",
    "clip = mpy.ImageSequenceClip(list(frames), fps=20)\n",
    "display.Image(as_gif(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import compose, initialize\n",
    "initialize(config_path=\"./conf\", job_name=\"test_app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import cv2\n",
    "\n",
    "\n",
    "# data loading\n",
    "def get_batch_grp(split, dataset, batch_size):\n",
    "    # generate a small batch of inputs x and targets y\n",
    "    data = dataset['train'] if split == 'train' else dataset['test']\n",
    "    ix = np.random.randint(int(len(data[\"img\"])), size=(batch_size,))\n",
    "    x = torch.tensor(data[\"img\"][ix], dtype=torch.float)\n",
    "    x_goal = torch.tensor(data[\"goal\"][ix], dtype=torch.long)\n",
    "    x_goal_img = torch.tensor(data[\"goal_img\"][ix], dtype=torch.float)\n",
    "    y = torch.tensor(data[\"action\"][ix], dtype=torch.float)\n",
    "    return x, x_goal, x_goal_img, y\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(model._cfg.eval_iters)\n",
    "        for k in range(model._cfg.eval_iters):\n",
    "            X, x_goal, x_goal_img, Y = get_batch_grp(split, model._dataset, model._cfg.batch_size)\n",
    "            logits, loss = model(X, x_goal, x_goal_img, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def get_patches_fast(images):\n",
    "    from einops import rearrange\n",
    "    batch_size, channels, height, width = images.shape\n",
    "    patch_size = height // 8 ## n_patches = 8\n",
    "\n",
    "    patches = rearrange(images, 'b (h p1) (w p2) c -> b (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size)\n",
    "    return patches\n",
    "\n",
    "def calc_positional_embeddings(sequence_length, d):\n",
    "    result = torch.ones(sequence_length, d)\n",
    "    for i in range(sequence_length):\n",
    "        for j in range(d):\n",
    "            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "    return result\n",
    "\n",
    "## This is an encoder head (full attention)\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        # self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B,T,C = x.shape\n",
    "        TODO\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        ### Block masked attention\n",
    "        wei = wei.masked_fill(mask == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, n_embd=n_embd, dropout=dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        out = torch.cat([h(x, mask) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x,)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head, dropout):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, n_embd=n_embd, dropout=dropout)\n",
    "        self.ffwd = FeedFoward(n_embd, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.sa(self.ln1(x), mask)\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GRP(nn.Module):\n",
    "  def __init__(self, dataset, cfg, mlp_ratio=4):\n",
    "    super(GRP, self).__init__()\n",
    "    self._dataset = dataset\n",
    "    self._cfg = cfg\n",
    "    TODO\n",
    "\n",
    "  def _init_weights(self, module):\n",
    "      if isinstance(module, nn.Linear):\n",
    "          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "          if module.bias is not None:\n",
    "              torch.nn.init.zeros_(module.bias)\n",
    "      elif isinstance(module, nn.Embedding):\n",
    "          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "  def forward(self, images, goals_txt, goal_imgs, targets=None):\n",
    "    # Dividing images into patches\n",
    "    n, c, h, w = images.shape\n",
    "    B, T = goals_txt.shape\n",
    "    TODO\n",
    "\n",
    "import hydra, json\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from hydra import compose, initialize\n",
    "\n",
    "# @hydra.main(config_path=\"conf\", config_name=\"grp-mini\")\n",
    "# @hydra.main(config_path=\"./conf\", config_name=\"bridge-64-light\")\n",
    "def my_main():\n",
    "    cfg = compose(config_path=\"./conf\", config_name=\"bridge-64-light\", overrides=[\"+env=absolute_path\"])\n",
    "\n",
    "    torch.manual_seed(cfg.r_seed)\n",
    "    print (\"cfg:\", OmegaConf.to_yaml(cfg))\n",
    "    torch.manual_seed(cfg.r_seed)\n",
    "    log_dir = hydra.core.hydra_config.HydraConfig.get().runtime.output_dir\n",
    "    print (\"cfg:\", OmegaConf.to_yaml(cfg))\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n",
    "    cfg.device = device\n",
    "    from datasets import load_dataset, load_from_disk\n",
    "\n",
    "    dataset = load_dataset(cfg.dataset.to_name, split='train')\n",
    "    print('Features:', dataset.features)\n",
    "\n",
    "    dataset_tmp = {\n",
    "        \"img\": np.array(dataset[\"img\"]),\n",
    "        \"action\": np.concatenate((np.array(dataset[\"action\"]) \n",
    "                                ,np.array(dataset[\"rotation_delta\"])\n",
    "                                ,np.array(dataset[\"open_gripper\"])\n",
    "                                ), axis=1),\n",
    "        \"goal_img\": np.array(dataset[\"goal_img\"]),\n",
    "        \"goal\": dataset[\"goal\"]\n",
    "    }\n",
    "    shortest_text_len = min([len(txt) for txt in dataset[\"goal\"]])\n",
    "    cfg.block_size = shortest_text_len\n",
    "\n",
    "    # here are all the unique characters that occur in this text\n",
    "    chars = sorted(list(set([item for row in dataset_tmp[\"goal\"] for item in row]))) ## Flatten to a long string\n",
    "    cfg.vocab_size = len(chars)\n",
    "    # create a mapping from characters to integers\n",
    "    stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "    itos = { i:ch for i,ch in enumerate(chars) }\n",
    "    encode_txt = lambda s: [stoi[c] for c in s] # text encoder to tokens: \n",
    "    decode_txy = lambda l: ''.join([itos[i] for i in l]) # token decoder to text: \n",
    "    print(\"vocab_size:\", cfg.vocab_size)\n",
    "    print(\"example text encode:\", encode_txt(dataset_tmp[\"goal\"][0]))\n",
    "\n",
    "    TODO\n",
    "\n",
    "    ## Get the actions and encode them to map to [-1, 1]\n",
    "    encode_state = lambda af:   ((af/(255.0)*2.0)-1.0).astype(np.float32) # encoder: take a float, output an integer\n",
    "    resize_state = lambda sf:   cv2.resize(np.array(sf, dtype=np.float32), (cfg.image_shape[0], cfg.image_shape[1]))  # resize state\n",
    "\n",
    "    dataset_tmp = {\n",
    "        \"img\": torch.tensor(encode_state(dataset_tmp[\"img\"])).to(device),\n",
    "        \"action\": torch.tensor(encode_action(dataset_tmp[\"action\"]), dtype=torch.float).to(device),            \n",
    "        \"goal_img\": torch.tensor(encode_state(dataset_tmp[\"goal_img\"])).to(device),\n",
    "        \"goal\": torch.tensor([encode_txt(goal[:cfg.block_size]) for goal in dataset_tmp[\"goal\"]]).to(device)\n",
    "    }\n",
    "\n",
    "    print(\"Dataset shape:\", len(dataset_tmp[\"img\"]))\n",
    "    dataset_tmp = {\"train\": dataset_tmp, \"test\": dataset_tmp} \n",
    "    if not cfg.testing:\n",
    "        import wandb\n",
    "        # start a new wandb run to track this script\n",
    "        wandb.init(\n",
    "            # set the wandb project where this run will be logged\n",
    "            project=cfg.experiment.project,\n",
    "\n",
    "            # track hyperparameters and run metadata\n",
    "            config= OmegaConf.to_container(cfg)\n",
    "        )\n",
    "        wandb.run.log_code(\".\")\n",
    "    model = GRP(dataset_tmp, cfg)\n",
    "    m = model.to(device)\n",
    "    print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "    # create a PyTorch optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate)\n",
    "    import torch.optim.lr_scheduler as lr_scheduler\n",
    "    scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=cfg.max_iters)\n",
    "\n",
    "    if cfg.simEval:\n",
    "        import simpler_env\n",
    "        from simpler_env.utils.env.observation_utils import get_image_from_maniskill2_obs_dict\n",
    "        task_name = \"widowx_carrot_on_plate\"  # @param [\"google_robot_pick_coke_can\", \"google_robot_move_near\", \"google_robot_open_drawer\", \"google_robot_close_drawer\", \"widowx_spoon_on_towel\", \"widowx_carrot_on_plate\", \"widowx_stack_cube\", \"widowx_put_eggplant_in_basket\"]\n",
    "        if 'env' in locals():\n",
    "            print(\"Closing existing env\")\n",
    "            env.close()\n",
    "            del env\n",
    "        env = simpler_env.make(task_name)\n",
    "        env_unwrapped = env.env.env.env ## Updated gymnasium wrapper adds lots of wrappers.\n",
    "\n",
    "    for iter in range(cfg.max_iters):\n",
    "\n",
    "        if iter % cfg.eval_interval == 0 or iter == cfg.max_iters - 1:\n",
    "            losses = estimate_loss(model)\n",
    "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "            if not cfg.testing:\n",
    "                wandb.log({\"train loss\": losses['train'], \"val loss\": losses['val']})\n",
    "\n",
    "            if cfg.simEval and (iter % cfg.eval_vid_iters == 0): ## Do this eval infrequently because it takes a fiar bit of compute\n",
    "                rewards = []\n",
    "                for j in range(cfg.sim.eval_episodes): ## Better to eval over a few different goal configurations\n",
    "                    obs, reset_info = env.reset()\n",
    "                    instruction = env_unwrapped.get_language_instruction()\n",
    "                    print(\"Reset info\", reset_info)\n",
    "                    print(\"Instruction\", instruction)\n",
    "                    frames = []\n",
    "                    done, truncated, timeLimit, t = False, False, 100, 0\n",
    "                    while not (done or truncated or (t > timeLimit)):\n",
    "                        # action[:3]: delta xyz; action[3:6]: delta rotation in axis-angle representation;\n",
    "                        # action[6:7]: gripper (the meaning of open / close depends on robot URDF)\n",
    "                        image = get_image_from_maniskill2_obs_dict(env_unwrapped, obs)\n",
    "                        image = image[:,:,:3] ## Remove last dimension of image color\n",
    "                        action, loss = model.forward(torch.tensor(np.array([encode_state(resize_state(image))])).to(device)\n",
    "                                            ,torch.tensor(np.array([encode_txt(instruction)[:cfg.block_size]])).to(device) ## There can be issues here if th text is shorter than any example in the dataset\n",
    "                                            ,torch.tensor(np.array([encode_state(resize_state(image))])).to(device) ## Not the correct goal image... Should mask this.\n",
    "                                            )\n",
    "                        # action = env.action_space.sample() # replace this with your policy inference\n",
    "                        if cfg.load_action_bounds:\n",
    "                            action = decode_action(action.cpu().detach().numpy()[0]) ## Add in the gripper close action\n",
    "                        else:\n",
    "                            action = np.concatenate((decode_action(action.cpu().detach().numpy()[0]), [0]), axis = -1) ## Add in the gripper close action\n",
    "                        obs, reward, done, truncated, info = env.step(action)\n",
    "                        reward = -np.linalg.norm(info[\"eof_to_obj1_diff\"])\n",
    "                        frames.append(image)\n",
    "                        rewards.append(reward)\n",
    "                        t=t+1\n",
    "                \n",
    "                episode_stats = info.get('episode_stats', {})\n",
    "                print(\"Episode stats\", episode_stats)\n",
    "                print(f\"avg reward {np.mean(rewards):.8f}\")\n",
    "                if not cfg.testing:\n",
    "                    wandb.log({\"avg reward\": np.mean(rewards)})\n",
    "                import moviepy.editor as mpy\n",
    "                clip = mpy.ImageSequenceClip(list(frames), fps=20)\n",
    "                clip.write_videofile(log_dir+\"/sim-env-\"+str(iter)+\".mp4\", fps=20)\n",
    "                if not cfg.testing:\n",
    "                    wandb.log({\"example\": wandb.Video(log_dir+\"/sim-env-\"+str(iter)+\".mp4\")})\n",
    "\n",
    "        # sample a batch of data\n",
    "        xb, xg, xgi, yb = get_batch_grp('train', dataset_tmp, cfg.batch_size)\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, xg, xgi, yb)\n",
    "        loss.backward()\n",
    "\n",
    "        if (iter + 1) % cfg.gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    if not cfg.testing:\n",
    "        wandb.finish()\n",
    "    return losses['val']\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = my_main()\n",
    "    print(\"results:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mini-grp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8wspFjLZmz4b",
        "outputId": "b220ef56-aba6-4940-af71-73cd340bf13c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'robot_learning_2025' already exists and is not an empty directory.\n",
            "/content/robot_learning_2025\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from -r hw2/requirements.txt (line 2)) (3.10.0)\n",
            "Collecting rlds (from -r hw2/requirements.txt (line 3))\n",
            "  Downloading rlds-0.1.8-py3-none-manylinux2010_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting dm-reverb (from -r hw2/requirements.txt (line 4))\n",
            "  Downloading dm_reverb-0.14.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (17 kB)\n",
            "Requirement already satisfied: moviepy==1.0.3 in /usr/local/lib/python3.11/dist-packages (from -r hw2/requirements.txt (line 5)) (1.0.3)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (from -r hw2/requirements.txt (line 6)) (1.26.4)\n",
            "Collecting pandas==2.2.1 (from -r hw2/requirements.txt (line 7))\n",
            "  Downloading pandas-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from -r hw2/requirements.txt (line 8)) (4.10.0.84)\n",
            "Collecting tensorflow==2.15.0 (from -r hw2/requirements.txt (line 9))\n",
            "  Downloading tensorflow-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting tensorflow_datasets==4.9.2 (from -r hw2/requirements.txt (line 10))\n",
            "  Downloading tensorflow_datasets-4.9.2-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: einops==0.8.0 in /usr/local/lib/python3.11/dist-packages (from -r hw2/requirements.txt (line 12)) (0.8.0)\n",
            "Collecting huggingface (from -r hw2/requirements.txt (line 13))\n",
            "  Downloading huggingface-0.0.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting datasets (from -r hw2/requirements.txt (line 14))\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting Pillow==9.4.0 (from -r hw2/requirements.txt (line 15))\n",
            "  Downloading Pillow-9.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (from -r hw2/requirements.txt (line 17)) (0.19.5)\n",
            "Collecting hydra-core==1.1.1 (from -r hw2/requirements.txt (line 18))\n",
            "  Downloading hydra_core-1.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting hydra-optuna-sweeper (from -r hw2/requirements.txt (line 19))\n",
            "  Downloading hydra_optuna_sweeper-1.2.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting hydra-submitit-launcher (from -r hw2/requirements.txt (line 20))\n",
            "  Downloading hydra_submitit_launcher-1.2.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from -r hw2/requirements.txt (line 21)) (4.47.1)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.11/dist-packages (from moviepy==1.0.3->-r hw2/requirements.txt (line 5)) (4.4.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.11/dist-packages (from moviepy==1.0.3->-r hw2/requirements.txt (line 5)) (2.36.1)\n",
            "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from moviepy==1.0.3->-r hw2/requirements.txt (line 5)) (0.6.0)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.11/dist-packages (from moviepy==1.0.3->-r hw2/requirements.txt (line 5)) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from moviepy==1.0.3->-r hw2/requirements.txt (line 5)) (2.32.3)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.11/dist-packages (from moviepy==1.0.3->-r hw2/requirements.txt (line 5)) (0.1.10)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.1->-r hw2/requirements.txt (line 7)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.1->-r hw2/requirements.txt (line 7)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.1->-r hw2/requirements.txt (line 7)) (2025.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0->-r hw2/requirements.txt (line 9)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0->-r hw2/requirements.txt (line 9)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0->-r hw2/requirements.txt (line 9)) (25.1.24)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0->-r hw2/requirements.txt (line 9)) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0->-r hw2/requirements.txt (line 9)) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0->-r hw2/requirements.txt (line 9)) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0->-r hw2/requirements.txt (line 9)) (18.1.1)\n",
            "Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0->-r hw2/requirements.txt (line 9))\n",
            "  Downloading ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0->-r hw2/requirements.txt (line 9)) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0->-r hw2/requirements.txt (line 9)) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0->-r hw2/requirements.txt (line 9)) (4.25.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0->-r hw2/requirements.txt (line 9)) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0->-r hw2/requirements.txt (line 9)) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0->-r hw2/requirements.txt (line 9)) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0->-r hw2/requirements.txt (line 9)) (4.12.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.0->-r hw2/requirements.txt (line 9))\n",
            "  Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0->-r hw2/requirements.txt (line 9)) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0->-r hw2/requirements.txt (line 9)) (1.70.0)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0->-r hw2/requirements.txt (line 9))\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0->-r hw2/requirements.txt (line 9))\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0->-r hw2/requirements.txt (line 9))\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: array-record in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets==4.9.2->-r hw2/requirements.txt (line 10)) (0.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets==4.9.2->-r hw2/requirements.txt (line 10)) (8.1.8)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets==4.9.2->-r hw2/requirements.txt (line 10)) (0.1.8)\n",
            "Requirement already satisfied: etils>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets==4.9.2->-r hw2/requirements.txt (line 10)) (1.11.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets==4.9.2->-r hw2/requirements.txt (line 10)) (2.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets==4.9.2->-r hw2/requirements.txt (line 10)) (5.9.5)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets==4.9.2->-r hw2/requirements.txt (line 10)) (1.16.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets==4.9.2->-r hw2/requirements.txt (line 10)) (0.10.2)\n",
            "Collecting omegaconf==2.1.* (from hydra-core==1.1.1->-r hw2/requirements.txt (line 18))\n",
            "  Downloading omegaconf-2.1.2-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core==1.1.1->-r hw2/requirements.txt (line 18))\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from omegaconf==2.1.*->hydra-core==1.1.1->-r hw2/requirements.txt (line 18)) (6.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r hw2/requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r hw2/requirements.txt (line 2)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r hw2/requirements.txt (line 2)) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r hw2/requirements.txt (line 2)) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r hw2/requirements.txt (line 2)) (3.2.1)\n",
            "Requirement already satisfied: portpicker in /usr/local/lib/python3.11/dist-packages (from dm-reverb->-r hw2/requirements.txt (line 4)) (1.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets->-r hw2/requirements.txt (line 14)) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->-r hw2/requirements.txt (line 14)) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->-r hw2/requirements.txt (line 14))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets->-r hw2/requirements.txt (line 14))\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets->-r hw2/requirements.txt (line 14))\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->-r hw2/requirements.txt (line 14))\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->-r hw2/requirements.txt (line 14)) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets->-r hw2/requirements.txt (line 14)) (0.27.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r hw2/requirements.txt (line 17)) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r hw2/requirements.txt (line 17)) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb->-r hw2/requirements.txt (line 17)) (4.3.6)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb->-r hw2/requirements.txt (line 17)) (2.10.6)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r hw2/requirements.txt (line 17)) (2.20.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb->-r hw2/requirements.txt (line 17)) (1.3.4)\n",
            "Collecting optuna<3.0.0,>=2.10.0 (from hydra-optuna-sweeper->-r hw2/requirements.txt (line 19))\n",
            "  Downloading optuna-2.10.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting submitit>=1.3.3 (from hydra-submitit-launcher->-r hw2/requirements.txt (line 20))\n",
            "  Downloading submitit-1.5.2-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->-r hw2/requirements.txt (line 21)) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->-r hw2/requirements.txt (line 21)) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers->-r hw2/requirements.txt (line 21)) (0.5.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0->-r hw2/requirements.txt (line 9)) (0.45.1)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets==4.9.2->-r hw2/requirements.txt (line 10)) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets==4.9.2->-r hw2/requirements.txt (line 10)) (3.21.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r hw2/requirements.txt (line 14)) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r hw2/requirements.txt (line 14)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r hw2/requirements.txt (line 14)) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r hw2/requirements.txt (line 14)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r hw2/requirements.txt (line 14)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r hw2/requirements.txt (line 14)) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r hw2/requirements.txt (line 14)) (1.18.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r hw2/requirements.txt (line 17)) (4.0.12)\n",
            "Collecting alembic (from optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper->-r hw2/requirements.txt (line 19))\n",
            "  Downloading alembic-1.14.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting cliff (from optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper->-r hw2/requirements.txt (line 19))\n",
            "  Downloading cliff-4.8.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting cmaes>=0.8.2 (from optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper->-r hw2/requirements.txt (line 19))\n",
            "  Downloading cmaes-0.11.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting colorlog (from optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper->-r hw2/requirements.txt (line 19))\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.11/dist-packages (from optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper->-r hw2/requirements.txt (line 19)) (1.13.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper->-r hw2/requirements.txt (line 19)) (2.0.37)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb->-r hw2/requirements.txt (line 17)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb->-r hw2/requirements.txt (line 17)) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3->-r hw2/requirements.txt (line 5)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3->-r hw2/requirements.txt (line 5)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3->-r hw2/requirements.txt (line 5)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3->-r hw2/requirements.txt (line 5)) (2024.12.14)\n",
            "Requirement already satisfied: cloudpickle>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from submitit>=1.3.3->hydra-submitit-launcher->-r hw2/requirements.txt (line 20)) (3.1.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0->-r hw2/requirements.txt (line 9)) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0->-r hw2/requirements.txt (line 9)) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0->-r hw2/requirements.txt (line 9)) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0->-r hw2/requirements.txt (line 9)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0->-r hw2/requirements.txt (line 9)) (3.1.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /usr/local/lib/python3.11/dist-packages (from tensorflow-metadata->tensorflow_datasets==4.9.2->-r hw2/requirements.txt (line 10)) (1.66.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r hw2/requirements.txt (line 17)) (5.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0->-r hw2/requirements.txt (line 9)) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0->-r hw2/requirements.txt (line 9)) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0->-r hw2/requirements.txt (line 9)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0->-r hw2/requirements.txt (line 9)) (1.3.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.1.0->optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper->-r hw2/requirements.txt (line 19)) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0->-r hw2/requirements.txt (line 9)) (3.0.2)\n",
            "Collecting Mako (from alembic->optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper->-r hw2/requirements.txt (line 19))\n",
            "  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from cliff->optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper->-r hw2/requirements.txt (line 19)) (3.13.0)\n",
            "Collecting autopage>=0.4.0 (from cliff->optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper->-r hw2/requirements.txt (line 19))\n",
            "  Downloading autopage-0.5.2-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting cmd2>=1.0.0 (from cliff->optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper->-r hw2/requirements.txt (line 19))\n",
            "  Downloading cmd2-2.5.11-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting stevedore>=2.0.1 (from cliff->optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper->-r hw2/requirements.txt (line 19))\n",
            "  Downloading stevedore-5.4.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: pyperclip>=1.8 in /usr/local/lib/python3.11/dist-packages (from cmd2>=1.0.0->cliff->optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper->-r hw2/requirements.txt (line 19)) (1.9.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.10 in /usr/local/lib/python3.11/dist-packages (from cmd2>=1.0.0->cliff->optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper->-r hw2/requirements.txt (line 19)) (0.2.13)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0->-r hw2/requirements.txt (line 9)) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0->-r hw2/requirements.txt (line 9)) (3.2.2)\n",
            "Collecting pbr>=2.0.0 (from stevedore>=2.0.1->cliff->optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper->-r hw2/requirements.txt (line 19))\n",
            "  Downloading pbr-6.1.1-py2.py3-none-any.whl.metadata (3.4 kB)\n",
            "Downloading pandas-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.3/475.3 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_datasets-4.9.2-py3-none-any.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Pillow-9.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hydra_core-1.1.1-py3-none-any.whl (145 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.8/145.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading omegaconf-2.1.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.7/74.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rlds-0.1.8-py3-none-manylinux2010_x86_64.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dm_reverb-0.14.0-cp311-cp311-manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hydra_optuna_sweeper-1.2.0-py3-none-any.whl (8.5 kB)\n",
            "Downloading hydra_submitit_launcher-1.2.0-py3-none-any.whl (5.2 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optuna-2.10.1-py3-none-any.whl (308 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.2/308.2 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading submitit-1.5.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.9/74.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cmaes-0.11.1-py3-none-any.whl (35 kB)\n",
            "Downloading alembic-1.14.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cliff-4.8.0-py3-none-any.whl (80 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading autopage-0.5.2-py3-none-any.whl (30 kB)\n",
            "Downloading cmd2-2.5.11-py3-none-any.whl (152 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.8/152.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stevedore-5.4.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Mako-1.3.9-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pbr-6.1.1-py2.py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.0/109.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141214 sha256=add8012dbc0c1d981d0afa4ea9d1514ce9aff1e46775ecec4ee22aa183f2fe2c\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/10/be/9a70640a3a60ed4a7e1a45e49bb9f58b04692d5d7b517bd39e\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: huggingface, antlr4-python3-runtime, xxhash, wrapt, tensorflow-estimator, submitit, rlds, Pillow, pbr, omegaconf, ml-dtypes, Mako, keras, fsspec, dill, colorlog, cmd2, cmaes, autopage, stevedore, pandas, multiprocess, hydra-core, dm-reverb, alembic, hydra-submitit-launcher, cliff, tensorflow_datasets, tensorboard, optuna, datasets, tensorflow, hydra-optuna-sweeper\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: pillow 11.1.0\n",
            "    Uninstalling pillow-11.1.0:\n",
            "      Successfully uninstalled pillow-11.1.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.8.0\n",
            "    Uninstalling keras-3.8.0:\n",
            "      Successfully uninstalled keras-3.8.0\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: tensorflow_datasets\n",
            "    Found existing installation: tensorflow-datasets 4.9.7\n",
            "    Uninstalling tensorflow-datasets-4.9.7:\n",
            "      Successfully uninstalled tensorflow-datasets-4.9.7\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.18.0\n",
            "    Uninstalling tensorflow-2.18.0:\n",
            "      Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.1 which is incompatible.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.15.0 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.15.0 which is incompatible.\n",
            "scikit-image 0.25.1 requires pillow>=10.1, but you have pillow 9.4.0 which is incompatible.\n",
            "google-genai 0.3.0 requires pillow<12.0.0,>=10.0.0, but you have pillow 9.4.0 which is incompatible.\n",
            "tensorstore 0.1.71 requires ml_dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Mako-1.3.9 Pillow-9.4.0 alembic-1.14.1 antlr4-python3-runtime-4.8 autopage-0.5.2 cliff-4.8.0 cmaes-0.11.1 cmd2-2.5.11 colorlog-6.9.0 datasets-3.2.0 dill-0.3.8 dm-reverb-0.14.0 fsspec-2024.9.0 huggingface-0.0.1 hydra-core-1.1.1 hydra-optuna-sweeper-1.2.0 hydra-submitit-launcher-1.2.0 keras-2.15.0 ml-dtypes-0.2.0 multiprocess-0.70.16 omegaconf-2.1.2 optuna-2.10.1 pandas-2.2.1 pbr-6.1.1 rlds-0.1.8 stevedore-5.4.0 submitit-1.5.2 tensorboard-2.15.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 tensorflow_datasets-4.9.2 wrapt-1.14.1 xxhash-3.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "pydevd_plugins",
                  "tensorflow_datasets",
                  "wrapt"
                ]
              },
              "id": "b6412189345c4978b55766722a65e9d1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'mediapy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-939cff098ca1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrlds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmediapy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmedia\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mediapy'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "!git clone https://github.com/milarobotlearningcourse/robot_learning_2025.git\n",
        "%cd robot_learning_2025/hw2\n",
        "!pip3 install -r requirements.txt\n",
        "\n",
        "# Install repo\n",
        "import cv2\n",
        "import tensorflow_datasets as tfds\n",
        "import tqdm\n",
        "import rlds, numpy as np\n",
        "import mediapy as media\n",
        "from PIL import Image\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8BDbUbYxmz4g",
        "outputId": "dee0e745-a276-4e21-853c-f286522436f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting previously unselected package libvulkan1:amd64.\n",
            "(Reading database ... 124926 files and directories currently installed.)\n",
            "Preparing to unpack .../libvulkan1_1.3.204.1-2_amd64.deb ...\n",
            "Unpacking libvulkan1:amd64 (1.3.204.1-2) ...\n",
            "Selecting previously unselected package libvulkan-dev:amd64.\n",
            "Preparing to unpack .../libvulkan-dev_1.3.204.1-2_amd64.deb ...\n",
            "Unpacking libvulkan-dev:amd64 (1.3.204.1-2) ...\n",
            "Selecting previously unselected package vulkan-tools.\n",
            "Preparing to unpack .../vulkan-tools_1.3.204.0+dfsg1-1_amd64.deb ...\n",
            "Unpacking vulkan-tools (1.3.204.0+dfsg1-1) ...\n",
            "Setting up libvulkan1:amd64 (1.3.204.1-2) ...\n",
            "Setting up libvulkan-dev:amd64 (1.3.204.1-2) ...\n",
            "Setting up vulkan-tools (1.3.204.0+dfsg1-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "## Install SimpleEnv for evaluation in the simulation\n",
        "# @title Install vulkan for rendering\n",
        "!apt-get install -yqq --no-install-recommends libvulkan-dev vulkan-tools\n",
        "# below fixes some bugs introduced by some recent Colab changes\n",
        "!mkdir -p /usr/share/vulkan/icd.d\n",
        "!wget -q -P /usr/share/vulkan/icd.d https://raw.githubusercontent.com/haosulab/ManiSkill/main/docker/nvidia_icd.json\n",
        "!wget -q -O /usr/share/glvnd/egl_vendor.d/10_nvidia.json https://raw.githubusercontent.com/haosulab/ManiSkill/main/docker/10_nvidia.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kDPEHkfSmz4h",
        "outputId": "c93920d1-df09-4c27-f574-845485602ef0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: [Loader Message] Code 0 : libGLX_nvidia.so.0: cannot open shared object file: No such file or directory\n",
            "Cannot create Vulkan instance.\n",
            "This problem is often caused by a faulty installation of the Vulkan driver or attempting to use a GPU that does not support Vulkan.\n",
            "ERROR at ./vulkaninfo/vulkaninfo.h:649:vkCreateInstance failed with ERROR_INCOMPATIBLE_DRIVER\n"
          ]
        }
      ],
      "source": [
        "# @title Make sure vulkan is installed correctly\n",
        "!vulkaninfo | head -n 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pnegZzP_mz4i",
        "outputId": "2a375708-e64b-41e4-9240-1364edfd8144",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SimplerEnv'...\n",
            "remote: Enumerating objects: 1570, done.\u001b[K\n",
            "remote: Counting objects: 100% (2/2), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 1570 (delta 0), reused 1 (delta 0), pack-reused 1568 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1570/1570), 16.89 MiB | 15.96 MiB/s, done.\n",
            "Resolving deltas: 100% (1104/1104), done.\n",
            "Submodule 'ManiSkill2_real2sim' (https://github.com/simpler-env/ManiSkill2_real2sim) registered for path 'ManiSkill2_real2sim'\n",
            "Cloning into '/content/robot_learning_2025/SimplerEnv/ManiSkill2_real2sim'...\n",
            "remote: Enumerating objects: 3451, done.        \n",
            "remote: Counting objects: 100% (65/65), done.        \n",
            "remote: Compressing objects: 100% (29/29), done.        \n",
            "remote: Total 3451 (delta 49), reused 36 (delta 36), pack-reused 3386 (from 2)        \n",
            "Receiving objects: 100% (3451/3451), 188.76 MiB | 22.66 MiB/s, done.\n",
            "Resolving deltas: 100% (1833/1833), done.\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 6 (delta 4), reused 6 (delta 4), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (6/6), 645 bytes | 215.00 KiB/s, done.\n",
            "From https://github.com/simpler-env/ManiSkill2_real2sim\n",
            " * branch            d32d54c8151fa87bc899f8a122a8c58619f85de6 -> FETCH_HEAD\n",
            "Submodule path 'ManiSkill2_real2sim': checked out 'd32d54c8151fa87bc899f8a122a8c58619f85de6'\n",
            "Obtaining file:///content/robot_learning_2025/SimplerEnv/ManiSkill2_real2sim\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from mani_skill2_real2sim==0.5.3) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from mani_skill2_real2sim==0.5.3) (1.13.1)\n",
            "Collecting gymnasium<1.0,>=0.28.1 (from mani_skill2_real2sim==0.5.3)\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting sapien==2.2.2 (from mani_skill2_real2sim==0.5.3)\n",
            "  Downloading sapien-2.2.2-cp311-cp311-manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from mani_skill2_real2sim==0.5.3) (3.12.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from mani_skill2_real2sim==0.5.3) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from mani_skill2_real2sim==0.5.3) (4.67.1)\n",
            "Requirement already satisfied: GitPython in /usr/local/lib/python3.11/dist-packages (from mani_skill2_real2sim==0.5.3) (3.1.44)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from mani_skill2_real2sim==0.5.3) (0.9.0)\n",
            "Requirement already satisfied: gdown>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from mani_skill2_real2sim==0.5.3) (5.2.0)\n",
            "Collecting transforms3d (from mani_skill2_real2sim==0.5.3)\n",
            "  Downloading transforms3d-0.4.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from mani_skill2_real2sim==0.5.3) (4.10.0.84)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (from mani_skill2_real2sim==0.5.3) (2.36.1)\n",
            "Collecting trimesh (from mani_skill2_real2sim==0.5.3)\n",
            "  Downloading trimesh-4.6.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting rtree (from mani_skill2_real2sim==0.5.3)\n",
            "  Downloading Rtree-1.3.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting ruckig (from mani_skill2_real2sim==0.5.3)\n",
            "  Downloading ruckig-0.14.0.tar.gz (870 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m870.7/870.7 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.22 in /usr/local/lib/python3.11/dist-packages (from sapien==2.2.2->mani_skill2_real2sim==0.5.3) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=4.6.0->mani_skill2_real2sim==0.5.3) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown>=4.6.0->mani_skill2_real2sim==0.5.3) (3.17.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.0,>=0.28.1->mani_skill2_real2sim==0.5.3) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.0,>=0.28.1->mani_skill2_real2sim==0.5.3) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.0,>=0.28.1->mani_skill2_real2sim==0.5.3) (0.0.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from GitPython->mani_skill2_real2sim==0.5.3) (4.0.12)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio->mani_skill2_real2sim==0.5.3) (9.4.0)\n",
            "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.11/dist-packages (from imageio[ffmpeg]->mani_skill2_real2sim==0.5.3) (0.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from imageio[ffmpeg]->mani_skill2_real2sim==0.5.3) (5.9.5)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->GitPython->mani_skill2_real2sim==0.5.3) (5.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22->sapien==2.2.2->mani_skill2_real2sim==0.5.3) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22->sapien==2.2.2->mani_skill2_real2sim==0.5.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22->sapien==2.2.2->mani_skill2_real2sim==0.5.3) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22->sapien==2.2.2->mani_skill2_real2sim==0.5.3) (2024.12.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.6.0->mani_skill2_real2sim==0.5.3) (2.6)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.6.0->mani_skill2_real2sim==0.5.3) (1.7.1)\n",
            "Downloading sapien-2.2.2-cp311-cp311-manylinux2014_x86_64.whl (39.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.1/39.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transforms3d-0.4.2-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Rtree-1.3.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (543 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m543.2/543.2 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trimesh-4.6.1-py3-none-any.whl (707 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m707.0/707.0 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: mani_skill2_real2sim, ruckig\n",
            "  Building editable for mani_skill2_real2sim (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mani_skill2_real2sim: filename=mani_skill2_real2sim-0.5.3-0.editable-py3-none-any.whl size=7635 sha256=791f5aecf099f9798fa824c032dbe23f1511cc5dd7570c6697019e7f8518eb5d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-q8ojvpeb/wheels/46/e1/c9/2ec815cbcbf9105ffdd4a68ba8ac67762173021965a51776f9\n",
            "  Building wheel for ruckig (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ruckig: filename=ruckig-0.14.0-cp311-cp311-linux_x86_64.whl size=921490 sha256=43cc4ac87a1679d747d6242c4b2613861831901efd108f80904c0e20af3b2d79\n",
            "  Stored in directory: /root/.cache/pip/wheels/b0/45/b5/86498b66f7491e7be64b8b8d7ace7507b3ad4f851c42ab3abc\n",
            "Successfully built mani_skill2_real2sim ruckig\n",
            "Installing collected packages: trimesh, transforms3d, ruckig, rtree, gymnasium, sapien, mani_skill2_real2sim\n",
            "  Attempting uninstall: gymnasium\n",
            "    Found existing installation: gymnasium 1.0.0\n",
            "    Uninstalling gymnasium-1.0.0:\n",
            "      Successfully uninstalled gymnasium-1.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gymnasium-0.29.1 mani_skill2_real2sim-0.5.3 rtree-1.3.0 ruckig-0.14.0 sapien-2.2.2 transforms3d-0.4.2 trimesh-4.6.1\n",
            "Obtaining file:///content/robot_learning_2025/SimplerEnv\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: simpler_env\n",
            "  Building editable for simpler_env (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for simpler_env: filename=simpler_env-0.0.1-0.editable-py3-none-any.whl size=3593 sha256=1cbc857f126fa64f6c7991ed41b833efebada076bbf534695b832ed5bb65aae8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vuy_8bga/wheels/de/0f/4e/f32705d41aad5fe570a4aec53f03e1bee55842c6e39dfad070\n",
            "Successfully built simpler_env\n",
            "Installing collected packages: simpler_env\n",
            "Successfully installed simpler_env-0.0.1\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/milarobotlearningcourse/SimplerEnv.git --recurse-submodules\n",
        "!pip install -e ./SimplerEnv/ManiSkill2_real2sim/\n",
        "!pip install -e ./SimplerEnv\n",
        "!mkdir ./SimplerEnv/checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_DBuWvaVmz4i",
        "outputId": "1bee4b7b-60eb-422e-a762-0d95d04bf9a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mutable default <class 'hydra.conf.JobConf.JobConfig.OverrideDirname'> for field override_dirname is not allowed: use default_factory",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-eb7134265852>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./conf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test_app\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/hydra/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Source of truth for Hydra's version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"1.1.1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMissingConfigException\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/hydra/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstantiate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_instantiate2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_locate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/hydra/_internal/instantiate/_instantiate2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0momegaconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_structured_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_locate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInstantiationException\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvertMode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTargetConf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/hydra/_internal/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_search_path_impl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfigSearchPathImpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_search_path\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfigSearchPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSearchPathQuery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_valid_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_config_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m from hydra.errors import (\n\u001b[1;32m     19\u001b[0m     \u001b[0mCompactHydraException\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/hydra/core/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeprecation_warning\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhydra_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHydraConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingleton\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSingleton\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHydraContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTaskFunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/hydra/core/hydra_config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0momegaconf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDictConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOmegaConf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHydraConf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingleton\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSingleton\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/hydra/conf/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# job runtime information will be populated here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdataclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mJobConf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0;31m# Job name, populated automatically unless specified by the user (in config or cli)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMISSING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/hydra/conf/__init__.py\u001b[0m in \u001b[0;36mJobConf\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# Job config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mdataclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mJobConfig\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mdataclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/dataclasses.py\u001b[0m in \u001b[0;36mdataclass\u001b[0;34m(cls, init, repr, eq, order, unsafe_hash, frozen, match_args, kw_only, slots, weakref_slot)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m     \u001b[0;31m# We're called as @dataclass without parens.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1232\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/dataclasses.py\u001b[0m in \u001b[0;36mwrap\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m         return _process_class(cls, init, repr, eq, order, unsafe_hash,\n\u001b[0m\u001b[1;32m   1223\u001b[0m                               \u001b[0mfrozen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslots\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m                               weakref_slot)\n",
            "\u001b[0;32m/usr/lib/python3.11/dataclasses.py\u001b[0m in \u001b[0;36m_process_class\u001b[0;34m(cls, init, repr, eq, order, unsafe_hash, frozen, match_args, kw_only, slots, weakref_slot)\u001b[0m\n\u001b[1;32m    956\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m             \u001b[0;31m# Otherwise it's a field of some type.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 958\u001b[0;31m             \u001b[0mcls_fields\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_field\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls_fields\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/dataclasses.py\u001b[0m in \u001b[0;36m_get_field\u001b[0;34m(cls, a_name, a_type, default_kw_only)\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;31m# not the instance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_field_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0m_FIELD\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__hash__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 815\u001b[0;31m         raise ValueError(f'mutable default {type(f.default)} for field '\n\u001b[0m\u001b[1;32m    816\u001b[0m                          f'{f.name} is not allowed: use default_factory')\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mutable default <class 'hydra.conf.JobConf.JobConfig.OverrideDirname'> for field override_dirname is not allowed: use default_factory"
          ]
        }
      ],
      "source": [
        "from hydra import compose, initialize\n",
        "initialize(config_path=\"./conf\", job_name=\"test_app\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vP2vEuxVmz4j",
        "outputId": "a1e67310-527d-4da0-c3cc-30e767f74b2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mutable default <class 'hydra.conf.JobConf.JobConfig.OverrideDirname'> for field override_dirname is not allowed: use default_factory",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-b9bc87a07acf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0mTODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0momegaconf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDictConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOmegaConf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/hydra/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Source of truth for Hydra's version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"1.1.1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMissingConfigException\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/hydra/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstantiate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_instantiate2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_locate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/hydra/_internal/instantiate/_instantiate2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0momegaconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_structured_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_locate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInstantiationException\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvertMode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTargetConf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/hydra/_internal/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_search_path_impl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfigSearchPathImpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_search_path\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfigSearchPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSearchPathQuery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_valid_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_config_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m from hydra.errors import (\n\u001b[1;32m     19\u001b[0m     \u001b[0mCompactHydraException\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/hydra/core/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeprecation_warning\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhydra_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHydraConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingleton\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSingleton\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHydraContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTaskFunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/hydra/core/hydra_config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0momegaconf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDictConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOmegaConf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHydraConf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingleton\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSingleton\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/hydra/conf/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# job runtime information will be populated here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdataclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mJobConf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0;31m# Job name, populated automatically unless specified by the user (in config or cli)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMISSING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/hydra/conf/__init__.py\u001b[0m in \u001b[0;36mJobConf\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# Job config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mdataclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mJobConfig\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mdataclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/dataclasses.py\u001b[0m in \u001b[0;36mdataclass\u001b[0;34m(cls, init, repr, eq, order, unsafe_hash, frozen, match_args, kw_only, slots, weakref_slot)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m     \u001b[0;31m# We're called as @dataclass without parens.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1232\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/dataclasses.py\u001b[0m in \u001b[0;36mwrap\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m         return _process_class(cls, init, repr, eq, order, unsafe_hash,\n\u001b[0m\u001b[1;32m   1223\u001b[0m                               \u001b[0mfrozen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslots\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m                               weakref_slot)\n",
            "\u001b[0;32m/usr/lib/python3.11/dataclasses.py\u001b[0m in \u001b[0;36m_process_class\u001b[0;34m(cls, init, repr, eq, order, unsafe_hash, frozen, match_args, kw_only, slots, weakref_slot)\u001b[0m\n\u001b[1;32m    956\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m             \u001b[0;31m# Otherwise it's a field of some type.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 958\u001b[0;31m             \u001b[0mcls_fields\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_field\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls_fields\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/dataclasses.py\u001b[0m in \u001b[0;36m_get_field\u001b[0;34m(cls, a_name, a_type, default_kw_only)\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;31m# not the instance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_field_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0m_FIELD\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__hash__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 815\u001b[0;31m         raise ValueError(f'mutable default {type(f.default)} for field '\n\u001b[0m\u001b[1;32m    816\u001b[0m                          f'{f.name} is not allowed: use default_factory')\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mutable default <class 'hydra.conf.JobConf.JobConfig.OverrideDirname'> for field override_dirname is not allowed: use default_factory"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "import cv2\n",
        "\n",
        "\n",
        "# data loading\n",
        "def get_batch_grp(split, dataset, batch_size):\n",
        "    # generate a small batch of inputs x and targets y\n",
        "    data = dataset['train'] if split == 'train' else dataset['test']\n",
        "    ix = np.random.randint(int(len(data[\"img\"])), size=(batch_size,))\n",
        "    x = torch.tensor(data[\"img\"][ix], dtype=torch.float)\n",
        "    x_goal = torch.tensor(data[\"goal\"][ix], dtype=torch.long)\n",
        "    x_goal_img = torch.tensor(data[\"goal_img\"][ix], dtype=torch.float)\n",
        "    y = torch.tensor(data[\"action\"][ix], dtype=torch.float)\n",
        "    return x, x_goal, x_goal_img, y\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(model._cfg.eval_iters)\n",
        "        for k in range(model._cfg.eval_iters):\n",
        "            X, x_goal, x_goal_img, Y = get_batch_grp(split, model._dataset, model._cfg.batch_size)\n",
        "            logits, loss = model(X, x_goal, x_goal_img, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "def get_patches_fast(images):\n",
        "    from einops import rearrange\n",
        "    batch_size, channels, height, width = images.shape\n",
        "    patch_size = height // 8 ## n_patches = 8\n",
        "\n",
        "    patches = rearrange(images, 'b (h p1) (w p2) c -> b (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size)\n",
        "    return patches\n",
        "\n",
        "def calc_positional_embeddings(sequence_length, d):\n",
        "    result = torch.ones(sequence_length, d)\n",
        "    for i in range(sequence_length):\n",
        "        for j in range(d):\n",
        "            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n",
        "    return result\n",
        "\n",
        "## This is an encoder head (full attention)\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size, n_embd, dropout):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        # self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        B,T,C = x.shape\n",
        "        TODO\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        ### Block masked attention\n",
        "        wei = wei.masked_fill(mask == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size, n_embd, dropout):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size, n_embd=n_embd, dropout=dropout) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        out = torch.cat([h(x, mask) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, dropout):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x,)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head, dropout):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size, n_embd=n_embd, dropout=dropout)\n",
        "        self.ffwd = FeedFoward(n_embd, dropout)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = x + self.sa(self.ln1(x), mask)\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GRP(nn.Module):\n",
        "  def __init__(self, dataset, cfg, mlp_ratio=4):\n",
        "    super(GRP, self).__init__()\n",
        "    self._dataset = dataset\n",
        "    self._cfg = cfg\n",
        "    TODO\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "      if isinstance(module, nn.Linear):\n",
        "          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "          if module.bias is not None:\n",
        "              torch.nn.init.zeros_(module.bias)\n",
        "      elif isinstance(module, nn.Embedding):\n",
        "          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "  def forward(self, images, goals_txt, goal_imgs, targets=None):\n",
        "    # Dividing images into patches\n",
        "    n, c, h, w = images.shape\n",
        "    B, T = goals_txt.shape\n",
        "    TODO\n",
        "\n",
        "import hydra, json\n",
        "from omegaconf import DictConfig, OmegaConf\n",
        "from hydra import compose, initialize\n",
        "\n",
        "# @hydra.main(config_path=\"conf\", config_name=\"grp-mini\")\n",
        "# @hydra.main(config_path=\"./conf\", config_name=\"bridge-64-light\")\n",
        "def my_main():\n",
        "    cfg = compose(config_path=\"./conf\", config_name=\"bridge-64-light\", overrides=[\"+env=absolute_path\"])\n",
        "\n",
        "    torch.manual_seed(cfg.r_seed)\n",
        "    print (\"cfg:\", OmegaConf.to_yaml(cfg))\n",
        "    torch.manual_seed(cfg.r_seed)\n",
        "    log_dir = hydra.core.hydra_config.HydraConfig.get().runtime.output_dir\n",
        "    print (\"cfg:\", OmegaConf.to_yaml(cfg))\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n",
        "    cfg.device = device\n",
        "    from datasets import load_dataset, load_from_disk\n",
        "\n",
        "    dataset = load_dataset(cfg.dataset.to_name, split='train')\n",
        "    print('Features:', dataset.features)\n",
        "\n",
        "    dataset_tmp = {\n",
        "        \"img\": np.array(dataset[\"img\"]),\n",
        "        \"action\": np.concatenate((np.array(dataset[\"action\"])\n",
        "                                ,np.array(dataset[\"rotation_delta\"])\n",
        "                                ,np.array(dataset[\"open_gripper\"])\n",
        "                                ), axis=1),\n",
        "        \"goal_img\": np.array(dataset[\"goal_img\"]),\n",
        "        \"goal\": dataset[\"goal\"]\n",
        "    }\n",
        "    shortest_text_len = min([len(txt) for txt in dataset[\"goal\"]])\n",
        "    cfg.block_size = shortest_text_len\n",
        "\n",
        "    # here are all the unique characters that occur in this text\n",
        "    chars = sorted(list(set([item for row in dataset_tmp[\"goal\"] for item in row]))) ## Flatten to a long string\n",
        "    cfg.vocab_size = len(chars)\n",
        "    # create a mapping from characters to integers\n",
        "    stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "    itos = { i:ch for i,ch in enumerate(chars) }\n",
        "    encode_txt = lambda s: [stoi[c] for c in s] # text encoder to tokens:\n",
        "    decode_txy = lambda l: ''.join([itos[i] for i in l]) # token decoder to text:\n",
        "    print(\"vocab_size:\", cfg.vocab_size)\n",
        "    print(\"example text encode:\", encode_txt(dataset_tmp[\"goal\"][0]))\n",
        "\n",
        "    TODO\n",
        "\n",
        "    ## Get the actions and encode them to map to [-1, 1]\n",
        "    encode_state = lambda af:   ((af/(255.0)*2.0)-1.0).astype(np.float32) # encoder: take a float, output an integer\n",
        "    resize_state = lambda sf:   cv2.resize(np.array(sf, dtype=np.float32), (cfg.image_shape[0], cfg.image_shape[1]))  # resize state\n",
        "\n",
        "    dataset_tmp = {\n",
        "        \"img\": torch.tensor(encode_state(dataset_tmp[\"img\"])).to(device),\n",
        "        \"action\": torch.tensor(encode_action(dataset_tmp[\"action\"]), dtype=torch.float).to(device),\n",
        "        \"goal_img\": torch.tensor(encode_state(dataset_tmp[\"goal_img\"])).to(device),\n",
        "        \"goal\": torch.tensor([encode_txt(goal[:cfg.block_size]) for goal in dataset_tmp[\"goal\"]]).to(device)\n",
        "    }\n",
        "\n",
        "    print(\"Dataset shape:\", len(dataset_tmp[\"img\"]))\n",
        "    dataset_tmp = {\"train\": dataset_tmp, \"test\": dataset_tmp}\n",
        "    if not cfg.testing:\n",
        "        import wandb\n",
        "        # start a new wandb run to track this script\n",
        "        wandb.init(\n",
        "            # set the wandb project where this run will be logged\n",
        "            project=cfg.experiment.project,\n",
        "\n",
        "            # track hyperparameters and run metadata\n",
        "            config= OmegaConf.to_container(cfg)\n",
        "        )\n",
        "        wandb.run.log_code(\".\")\n",
        "    model = GRP(dataset_tmp, cfg)\n",
        "    m = model.to(device)\n",
        "    print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "    # create a PyTorch optimizer\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate)\n",
        "    import torch.optim.lr_scheduler as lr_scheduler\n",
        "    scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=cfg.max_iters)\n",
        "\n",
        "    if cfg.simEval:\n",
        "        import simpler_env\n",
        "        from simpler_env.utils.env.observation_utils import get_image_from_maniskill2_obs_dict\n",
        "        task_name = \"widowx_carrot_on_plate\"  # @param [\"google_robot_pick_coke_can\", \"google_robot_move_near\", \"google_robot_open_drawer\", \"google_robot_close_drawer\", \"widowx_spoon_on_towel\", \"widowx_carrot_on_plate\", \"widowx_stack_cube\", \"widowx_put_eggplant_in_basket\"]\n",
        "        if 'env' in locals():\n",
        "            print(\"Closing existing env\")\n",
        "            env.close()\n",
        "            del env\n",
        "        env = simpler_env.make(task_name)\n",
        "        env_unwrapped = env.env.env.env ## Updated gymnasium wrapper adds lots of wrappers.\n",
        "\n",
        "    for iter in range(cfg.max_iters):\n",
        "\n",
        "        if iter % cfg.eval_interval == 0 or iter == cfg.max_iters - 1:\n",
        "            losses = estimate_loss(model)\n",
        "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "            if not cfg.testing:\n",
        "                wandb.log({\"train loss\": losses['train'], \"val loss\": losses['val']})\n",
        "\n",
        "            if cfg.simEval and (iter % cfg.eval_vid_iters == 0): ## Do this eval infrequently because it takes a fiar bit of compute\n",
        "                rewards = []\n",
        "                for j in range(cfg.sim.eval_episodes): ## Better to eval over a few different goal configurations\n",
        "                    obs, reset_info = env.reset()\n",
        "                    instruction = env_unwrapped.get_language_instruction()\n",
        "                    print(\"Reset info\", reset_info)\n",
        "                    print(\"Instruction\", instruction)\n",
        "                    frames = []\n",
        "                    done, truncated, timeLimit, t = False, False, 100, 0\n",
        "                    while not (done or truncated or (t > timeLimit)):\n",
        "                        # action[:3]: delta xyz; action[3:6]: delta rotation in axis-angle representation;\n",
        "                        # action[6:7]: gripper (the meaning of open / close depends on robot URDF)\n",
        "                        image = get_image_from_maniskill2_obs_dict(env_unwrapped, obs)\n",
        "                        image = image[:,:,:3] ## Remove last dimension of image color\n",
        "                        action, loss = model.forward(torch.tensor(np.array([encode_state(resize_state(image))])).to(device)\n",
        "                                            ,torch.tensor(np.array([encode_txt(instruction)[:cfg.block_size]])).to(device) ## There can be issues here if th text is shorter than any example in the dataset\n",
        "                                            ,torch.tensor(np.array([encode_state(resize_state(image))])).to(device) ## Not the correct goal image... Should mask this.\n",
        "                                            )\n",
        "                        # action = env.action_space.sample() # replace this with your policy inference\n",
        "                        if cfg.load_action_bounds:\n",
        "                            action = decode_action(action.cpu().detach().numpy()[0]) ## Add in the gripper close action\n",
        "                        else:\n",
        "                            action = np.concatenate((decode_action(action.cpu().detach().numpy()[0]), [0]), axis = -1) ## Add in the gripper close action\n",
        "                        obs, reward, done, truncated, info = env.step(action)\n",
        "                        reward = -np.linalg.norm(info[\"eof_to_obj1_diff\"])\n",
        "                        frames.append(image)\n",
        "                        rewards.append(reward)\n",
        "                        t=t+1\n",
        "\n",
        "                episode_stats = info.get('episode_stats', {})\n",
        "                print(\"Episode stats\", episode_stats)\n",
        "                print(f\"avg reward {np.mean(rewards):.8f}\")\n",
        "                if not cfg.testing:\n",
        "                    wandb.log({\"avg reward\": np.mean(rewards)})\n",
        "                import moviepy.editor as mpy\n",
        "                clip = mpy.ImageSequenceClip(list(frames), fps=20)\n",
        "                clip.write_videofile(log_dir+\"/sim-env-\"+str(iter)+\".mp4\", fps=20)\n",
        "                if not cfg.testing:\n",
        "                    wandb.log({\"example\": wandb.Video(log_dir+\"/sim-env-\"+str(iter)+\".mp4\")})\n",
        "\n",
        "        # sample a batch of data\n",
        "        xb, xg, xgi, yb = get_batch_grp('train', dataset_tmp, cfg.batch_size)\n",
        "\n",
        "        # evaluate the loss\n",
        "        logits, loss = model(xb, xg, xgi, yb)\n",
        "        loss.backward()\n",
        "\n",
        "        if (iter + 1) % cfg.gradient_accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    if not cfg.testing:\n",
        "        wandb.finish()\n",
        "    return losses['val']\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = my_main()\n",
        "    print(\"results:\", results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcyRcXPCmz4k"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mini-grp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
>>>>>>> 403e46fde07ea450201b9c2b3d830ec344d91be2
