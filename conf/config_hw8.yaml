env:
    # env_name: "PointmassHard-v0" # ['PointmassEasy-v0', 'PointmassMedium-v0', 'PointmassHard-v0', 'PointmassVeryHard-v0', 'DrunkSpider-v0']
    #env_name: "MsPacman-v0"
    env_name: "LunarLander-v3"
    max_episode_length: 200
    exp_name: 'LunarLander-v3-double-q'
    atari: True

alg:
    # rl_alg: 'explore_or_exploit' ## RL training algorithm ['explore_exploit', 'cql', 'safe_rl']
    rl_alg: 'dqn'
    n_iter: 50000
    batch_size: 128        ## The min amount of experience to collect before a training update
    batch_size: 128        ## The min amount of experience to collect before a training update
    train_batch_size: 256  ## training batch size used for computing gradients of q function or policy
    eval_batch_size: 1000  ## How much experience should be collected over the environment to evaluate the average reward of a policy
    use_rnd: False
    num_exploration_steps: 10000
    unsupervised_exploration: False
    offline_exploitation: True
    cql_alpha: 0.0
    exploit_rew_shift: 0.0
    exploit_rew_scale: 1.0
    rnd_output_size: 5
    rnd_n_layers: 2
    rnd_size: 400
    use_boltzmann: True
    no_gpu: False
    gpu_id: 0
    double_q: True
    # num_agent_train_steps_per_iter: 1 ## Number of training updates after #batch_size experience is collected. 
    num_agent_train_steps_per_iter: 4 ## Number of training updates after #batch_size experience is collected. 

    num_critic_updates_per_agent_update: 1 ## Number of training updates after #batch_size experience is collected.
    # learning_starts: 2000  ## How much initial experience to collect before training begins
    learning_starts: 2000
    learning_freq: 1 
    target_update_freq: 3000
    exploration_schedule: 0
    optimizer_spec:  0
    replay_buffer_size: 1000000
    # frame_history_len: 1
    frame_history_len: 1
    gamma: 0.99
    n_layers_critic: 2
    size_hidden_critic: 64
    critic_learning_rate: 1e-3
    eps: 0.2
    n_layers: 2
    size: 64
    learning_rate: 1e-3
    ob_dim: 0             # do not modify
    ac_dim: 0             # do not modify
    batch_size_initial: 0 # do not modify
    discrete: True
    grad_norm_clipping: True
    network:
        layer_sizes: [ 64, 64 ]
        activations: [ "tanh", "tanh" ]
        output_activation: "identity"

logging:
    video_log_freq: -1 # How often to generate a video to log/
    scalar_log_freq: 1 # How often to log training information and run evaluation during training.
    save_params: false # Should the parameters given to the script be saved? (Always...)
    random_seed: 1234
    logdir: ""
    eval_frequency: 1000